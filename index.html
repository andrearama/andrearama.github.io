<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Andrea Ramazzina</title>

    <meta name="author" content="Andrea Ramazzina">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Andrea Ramazzina
                </p>
                <p>Since 2021, I'm a PhD student at <a href="https://group.mercedes-benz.com/en/">Mercedes-Benz</a> and <a href="https://www.mpi-inf.mpg.de/home">Max Plank Institute of Informatics</a>, supervised by Felix Heide and Mario Bijelic.
		 My main research focus is on understanding the world through generative models.  	
                </p>
		      
                <p>Before that, from 2019 to 2021 I worked at <a href="volvoautonomoussolutions.com/en-en/">Volvo Autonomous Solutions</a> developing their V&V analytics and other data science projects.
		Here, I was also part of the <a href="https://www.volvogroup.com/en/careers/students-and-graduates/engineering-graduate-program.html">Engineering Graduate Program</a> 2019.
                </p>
		      
		<p>I have a MS in Physics of Complex Systems from <a href="https://www.chalmers.se/en/education/find-masters-programme/complex-adaptive-systems-msc/">Chalmers University of Technology</a> where I worked with
		<a href="https://scholar.google.com/citations?user=P_w6UgMAAAAJ&hl=en">Fredrik Kahl</a>, and a double BS degree from 
		<a href="https://www.polimi.it/">Politecnico di Milano</a> and <a href="https://en.tongji.edu.cn/p/#/">Tongji University</a>.
		During my studies I was recipient of the PoliTong Scholarship (2015), Adlerbert Hospitality Foundation Scholarship (2018) and Taiwan Elite Internship Program Scholarship (2018).
                </p>      
                <p style="text-align:center">
                  <a href="mailto:andrea_ramazzina@hotmail.it">Email</a> &nbsp;/&nbsp;
                  <a href="data/CURRICULUM VITAE_Long.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=dtLybe4AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/andrea-r-672234107/">Linkedin</a> &nbsp;/&nbsp;
		  <a href="https://github.com/andrearama/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profilePic.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profilePicCropped.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision, machine learning, optimization, and complex adaptive systems.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gatedfields_img.png" alt="gatedfields" width="200" height="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ramazzina_Gated_Fields_Learning_Scene_Reconstruction_from_Gated_Videos_CVPR_2024_paper.pdf">
                  <span class="papertitle">Gated Fields: Learning Scene Reconstruction from Gated Videos</span>
                </a>
                <br>
                <strong>Andrea Ramazzina*</strong>, <a href="https://scholar.google.com/citations?user=ZeHkzXwAAAAJ">Stefanie Walz*</a>, <a href="https://scholar.google.com/citations?user=aJV5jRYAAAAJ">Mario Bijelic</a>, <a href="https://scholar.google.com/citations?user=8BXZvdwAAAAJ&hl=en&oi=ao">Pragyan Dahal</a>, <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
                <br>
                <em>CVPR</em>, 2024 <br>
                <a href="https://light.princeton.edu/publication/gatedfields/">Project Page</a> / 
                <a href="https://light.princeton.edu/publication/gatedfields/">Code</a> / 
                <a href="https://light.princeton.edu/wp-content/uploads/2024/04/GatedLiDARNerf_compressed.pdf">PDF</a>
                <br>

                <p>We propose a neural rendering approach that seamlessly incorporates time-gated capture and illumination. Our method exploits the intrinsic depth cues in the gated videos, achieving precise and dense geometry reconstruction irrespective of ambient illumination conditions.</p>
              </td>
            </tr>		  

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/HINT_img.png" alt="hint" width="200" height="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2405.19712">
                  <span class="papertitle">Learning Complete Human Neural Representations from Limited Viewpoints</span>
                </a>
                <br>
                <a href="https://alexdruso.github.io/">Alessandro Sanvito*</a>, <strong>Andrea Ramazzina*</strong>, <a href="https://scholar.google.com/citations?user=ZeHkzXwAAAAJ">Stefanie Walz</a>, <a href="https://scholar.google.com/citations?user=aJV5jRYAAAAJ">Mario Bijelic</a>, <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
                <br>
                <em>IEEE IV</em>, 2024 <br>
                <a href="https://arxiv.org/abs/2405.19712">ArXiv</a>
                <br>

                <p>We introduce HINT, a novel method able to learn a robust representation of a human captured only in a limited range of views augmenting existing sequences with novel views of the person and poses.</p>
              </td>
            </tr>			  

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/Radar_Fields.jpg" alt="hint" width="200" height="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2405.04662">
                  <span class="papertitle">Radar Fields: Frequency-Space Neural Scene Representations for FMCW Radar</span>
                </a>
                <br>
                <a href="">David Borts</a>, <a href="">Erich Liang</a>, <a href="">Tim Brödermann</a>, <strong>Andrea Ramazzina</strong>, <a href="https://scholar.google.com/citations?user=ZeHkzXwAAAAJ">Stefanie Walz</a>, <a href="">Edoardo Palladin</a>, <a href="">David Bruggemann</a>, <a href="https://scholar.google.com/citations?user=gyF5LmoAAAAJ&hl=en">Christos Sakaridis</a>, <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en">Luc Van Gool</a>, <a href="https://scholar.google.com/citations?user=aJV5jRYAAAAJ">Mario Bijelic</a>, <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
                <br>
                <em>SIGGRAPH</em>, 2024 <br>
                <a href="https://light.princeton.edu/publication/radarfields/">Project Page</a> / 	
                <a href="https://dl.acm.org/doi/10.1145/3641519.3657510">ACM</a> / 
		<a href="https://github.com/princeton-computational-imaging/RadarFields">Code</a> / 
                <a href="https://arxiv.org/abs/2405.04662">ArXiv</a>
                <br>

                <p>We introduce Radar Fields - a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy.</p>
              </td>
            </tr>			            

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/scatternerf_img.png" alt="scatternerf" width="200" height="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ramazzina_ScatterNeRF_Seeing_Through_Fog_with_Physically-Based_Inverse_Neural_Rendering_ICCV_2023_paper.pdf">
                  <span class="papertitle">ScatterNeRF: Seeing Through Fog with Physically-Based Inverse Neural Rendering</span>
                </a>
                <br>
                <strong>Andrea Ramazzina</strong>, <a href="https://scholar.google.com/citations?user=aJV5jRYAAAAJ">Mario Bijelic</a>, <a href="https://scholar.google.com/citations?user=ZeHkzXwAAAAJ">Stefanie Walz</a>, <a href="https://alexdruso.github.io/">Alessandro Sanvito</a>, <a href="#">Dominik Scheuble</a>, <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
                <br>
                <em>ICCV</em>, 2023 <br>
                <a href="https://light.princeton.edu/publication/scatternerf/">Project Page</a> / 
                <a href="https://github.com/princeton-computational-imaging/scatternerf">Code</a> / 
                <a href="https://arxiv.org/abs/2305.02103">ArXiv</a>
                <br>

                <p>We learn a foggy scene from a single video, disjointly representing scattering volume and scene objects through physics-inspired losses. This allows for accurate rendering of novel views with full control over the scattering volume, as well as a more accurate estimation of the 3D scene.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/gatedstereo_img.png" alt="clean-usnob" width="200" height="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Walz_Gated_Stereo_Joint_Depth_Estimation_From_Gated_and_Wide-Baseline_Active_CVPR_2023_paper.pdf">
                  <span class="papertitle">Gated Stereo: Joint Depth Estimation from Gated and Wide-Baseline Active Stereo Cues</span>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=ZeHkzXwAAAAJ">Stefanie Walz</a>,
                <a href="https://scholar.google.com/citations?user=aJV5jRYAAAAJ">Mario Bijelic</a>,
                <strong>Andrea Ramazzina</strong>, 
                <a href="#">Amanpreet Walia</a>,
                <a href="https://scholar.google.com/citations?user=TsINiJcAAAAJ&hl=en">Fahim Mannan</a>,
                <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>
                <br>
                <em>CVPR</em>, 2023 <b>(Highlight)</b> <br>  
                <a href="https://light.princeton.edu/publication/gatedstereo/">Project Page</a> / 
                <a href="https://github.com/princeton-computational-imaging/GatedStereo">Code</a> / 
                <a href="https://arxiv.org/abs/2305.12955">ArXiv</a>
                <br>

                <p>We propose Gated Stereo, a high-resolution and long-range depth estimation technique that operates on active gated stereo images.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/singleScattering.png" alt="clean-usnob" width="200" height="200">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.cs.princeton.edu/~fheide/papers/SourcesInFog.pdf">
                  <span class="papertitle">Single scattering models for radiative transfer of isotropic and cone-shaped light sources in fog</span>
                </a>
                <br>
                <a href="#">Simeon Geiger</a>,
                <a href="#">André Liemert</a>,
                <a href="#">Dominik Reitzle</a>,
                <a href="https://scholar.google.com/citations?user=aJV5jRYAAAAJ">Mario Bijelic</a>,
                <strong>Andrea Ramazzina</strong>, 
                <a href="#">Werner Ritter</a>,
                <a href="https://www.cs.princeton.edu/~fheide/">Felix Heide</a>,
                <a href="#">Alwin Kienle</a>
                <br>
                <em>Optics Express</em>, 2022 <br>  
                <a href="https://www.cs.princeton.edu/~fheide/papers/SourcesInFog.pdf">Paper</a>
                <br>

                <p>We present an improved solution to calculate the so called air-light integral that can be evaluated fast and robustly for an isotropic point source in homogeneous media.</p>
              </td>
            </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Previously Supervised Students</h2>
                <ul>
                  <li><a href="https://www.linkedin.com/in/alessandro-sanvito-07706114b/">Alessandro Sanvito</a> - M.Sc - Now at Optiver</li>
                  <li><a href="https://www.linkedin.com/in/tina-h%C3%B6flich-010502196/">Tina Höflich</a> - B.Sc - Now at Mercedes-Benz</li>
                </ul>
              </td>
            </tr>

          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
